# -*- coding: utf-8 -*-
"""Copy of corona.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kGeuRmSkF1clxNKIWYsZEan30DPmaJIG
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

#import warnings
#warnings.filterwarnings('ignore')

df = pd.read_csv('/content/corona.csv',encoding='ISO-8859-1')

df.head()

"""Data Preprocessing"""

# function to convert the data

def convert(x):
    if x=='TRUE':
        return 'true'
    elif x==True:
        return 'true'
    elif x=='FALSE' or x==False:
        return 'false'
    else:
        return x

# mapping the function

df['Cough_symptoms'] = df['Cough_symptoms'].map(convert)
df['Fever'] = df['Fever'].map(convert)
df['Sore_throat'] = df['Sore_throat'].map(convert)
df['Shortness_of_breath'] = df['Shortness_of_breath'].map(convert)
df['Headache'] = df['Headache'].map(convert)

df.info()

# replacing the none values with Nan values

df.replace({'None': np.nan}, inplace=True)

df.isnull().sum()

# Dropping the nan values from columns

df.dropna(subset=['Cough_symptoms','Fever','Sore_throat','Shortness_of_breath','Headache'],axis=0,inplace=True)

# dropping the age_60_above column

df.drop('Age_60_above',axis=1,inplace=True)

# # replacing the none values with Nan values

df.replace({'other': np.nan},inplace=True)

# Dropping the nan values from columns

df.dropna(subset=['Corona'],axis=0,inplace=True)

# converting the datatype as categorical

for i in df.columns:
    if i=='Ind_ID' or i == 'Test_date':
        pass
    else:
        df[i] = df[i].astype('category')

df.info()

"""Explorartory Data Analysis"""

sns.set_style('dark')

cols = ['Cough_symptoms', 'Fever', 'Sore_throat']

# Set up the figure and axes

fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))

# Plot univariate distributions for each column
for i, col in enumerate(cols):
    sns.countplot(data=df , x=col, ax=axes[i])
    axes[i].set_title(f'Distribution of {col}' , fontsize=18 )
    axes[i].set_ylabel('Count')
    axes[i].tick_params(axis='x')
    axes[i].grid(True)

plt.tight_layout()
plt.show()

cols = ['Shortness_of_breath','Headache',	'Corona']

# Set up the figure and axes

fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))

# Plot univariate distributions for each column
for i, col in enumerate(cols):
    sns.countplot(data=df , x=col, ax=axes[i])
    axes[i].set_title(f'Distribution of {col}' , fontsize=18 )
    axes[i].set_ylabel('Count')
    axes[i].tick_params(axis='x')
    axes[i].grid(True)

plt.tight_layout()
plt.show()

cols = ['Sex',	'Known_contact']

# Set up the figure and axes

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))

# Plot univariate distributions for each column
for i, col in enumerate(cols):
    sns.countplot(data=df , x=col, ax=axes[i])
    axes[i].set_title(f'Distribution of {col}' , fontsize=18 )
    axes[i].set_ylabel('Count')
    axes[i].tick_params(axis='x')
    axes[i].grid(True)

plt.tight_layout()
plt.show()

cols = ['Cough_symptoms', 'Fever', 'Sore_throat']

# Set up the figure and axes

fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))

# Plot univariate distributions for each column
for i, col in enumerate(cols):
    sns.countplot(data=df , x=col,hue='Corona', ax=axes[i]  , palette="Set2")
    axes[i].set_title(f'Distribution of {col}' , fontsize=18 )
    axes[i].set_ylabel('Count')
    axes[i].tick_params(axis='x')
    axes[i].grid(True)

plt.tight_layout()
plt.show()

cols = ['Shortness_of_breath','Headache']

# Set up the figure and axes

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))

# Plot univariate distributions for each column
for i, col in enumerate(cols):
    sns.countplot(data=df , x=col,hue='Corona', ax=axes[i]  , palette="Set2")
    axes[i].set_title(f'Distribution of {col}' , fontsize=18 )
    axes[i].set_ylabel('Count')
    axes[i].tick_params(axis='x')
    axes[i].grid(True)

plt.tight_layout()
plt.show()

cols = ['Sex',	'Known_contact']

# Set up the figure and axes

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))

# Plot univariate distributions for each column
for i, col in enumerate(cols):
    sns.countplot(data=df , x=col,hue='Corona', ax=axes[i]  , palette="Set2")
    axes[i].set_title(f'Distribution of {col}' , fontsize=12 )
    axes[i].set_ylabel('Count')
    axes[i].tick_params(axis='x')
    axes[i].grid(True)

plt.tight_layout()
plt.show()

"""Feature Transformation"""

# creating a copy of original dataframe

covid_df = df.copy(deep=True)

# Encoding

covid_df['Cough_symptoms'] = pd.get_dummies(covid_df['Cough_symptoms'],drop_first=True)  # to aviod multicolinearity

covid_df['Fever'] = pd.get_dummies(covid_df['Fever'],drop_first=True)

covid_df['Sore_throat'] = pd.get_dummies(covid_df['Sore_throat'],drop_first=True)

covid_df['Shortness_of_breath'] = pd.get_dummies(covid_df['Shortness_of_breath'],drop_first=True)

covid_df['Headache'] = pd.get_dummies(covid_df['Headache'],drop_first=True)

covid_df['Corona'] = pd.get_dummies(covid_df['Corona'],drop_first=True)

# encoding

covid_df['Sex'].replace({'male':0,'female':1},inplace=True)
covid_df['Known_contact'].replace({'Other':2,'Contact with confirmed':1,'Abroad':0},inplace=True)

covid_df.head()

covid_df.isnull().sum()

"""Missing Value Impuatation"""

from sklearn.impute import KNNImputer

# creating a copy of original dataframe

imputed_data = covid_df.copy(deep=True)

# applying KNN imputation

knn=KNNImputer(n_neighbors=5,weights='uniform')
columns=['Cough_symptoms', 'Fever', 'Sore_throat','Shortness_of_breath', 'Headache','Sex','Known_contact']
imputed=knn.fit_transform(covid_df[columns])

df1 = pd.DataFrame(imputed,columns=columns)

def change(x):
    if x>0.5:
        return 1
    elif x<0.5:
        return 0
    else:
        return x

df1['Sex'] = df1['Sex'].apply(change)

imputed_data['Sex'].iloc[:] =df1['Sex']

imputed_data.head()

imputed_data.info()

imputed_data['Sex'] = imputed_data['Sex'].astype('uint8')

# strong teh data in final df

final_data = imputed_data.astype('category')

final_data.info()

"""Feature Engineering"""

# separating features and target variable

features = final_data.drop(['Ind_ID','Corona'],axis=1)

target = final_data['Corona']

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# chi square method to select important k best features

selector = SelectKBest(score_func=chi2, k=6)

X_new = selector.fit_transform(features, target)

idxs_selected = selector.get_support(indices=True)

feat_names = features.columns[idxs_selected]

feat_names

from sklearn.model_selection import train_test_split

# splitting train test set

x_train,x_test,y_train,y_test = train_test_split(features,target,test_size=0.3,random_state=42)

y_test.value_counts()

y_train.value_counts()

"""Model Training"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, make_scorer, precision_score, recall_score, f1_score

lr = LogisticRegression(random_state=42)

lr.fit(x_train, y_train)
pred_lr = lr.predict(x_test)

accuracy_lr = accuracy_score(y_test, pred_lr)
print("Accuracy:", accuracy_lr * 100)

precision_lr = precision_score(y_test, pred_lr)
print("Precision:", precision_lr * 100)

# Recall
recall_lr = recall_score(y_test, pred_lr)
print("Recall:", recall_lr * 100)

# F1 score
f1_lr = f1_score(y_test, pred_lr)
print("F1 Score:", f1_lr * 100)

# Logistic Regression Grid Search
param_grid_lr = {'penalty': ['l1', 'l2', 'elasticnet', 'none'],
                 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}

import warnings
warnings.filterwarnings('ignore')

"""Hyper Parameter Tuning"""

scorer_lr = make_scorer(accuracy_score)

grid_obj_lr = GridSearchCV(lr, param_grid_lr, scoring=scorer_lr)

grid_fit_lr = grid_obj_lr.fit(x_train, y_train)

best_params_lr = grid_fit_lr.best_params_
print("Best Parameters for Logistic Regression:", best_params_lr)

# Logistic Regression with Best Parameters
lr_best = LogisticRegression(random_state=42, **best_params_lr)

lr_best.fit(x_train, y_train)

y_pred_lr = lr_best.predict(x_test)

import pickle

# Save the Logistic Regression model to a file using pickle
with open('logistic_regression_model.pkl', 'wb') as file:
    pickle.dump(lr_best, file)

# To load the Logistic Regression model later
with open('logistic_regression_model.pkl', 'rb') as file:
    loaded_lr_model = pickle.load(file,encoding='ISO-8859-1')

import chardet

# Specify the path to your file
file_path = '/content/corona.csv'

# Open the file in binary mode and read a portion of it for detection
with open(file_path, 'rb') as f:
    result = chardet.detect(f.read(10000))  # Read the first 10,000 bytes for detection

# Retrieve the detected encoding
detected_encoding = result['encoding']
confidence = result['confidence']

print(f"Detected encoding: {detected_encoding} with confidence: {confidence}")

# Use the detected encoding to read the file
with open(file_path, 'r', encoding=detected_encoding) as f:
    content = f.read()

# Now 'content' contains the decoded text using the detected encoding